/*
 * This file was automatically generated by EvoSuite
 * Sun Nov 06 02:44:46 GMT 2022
 */

package burlap.behavior.stochasticgames.madynamicprogramming.policies;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import burlap.behavior.learningrate.LearningRate;
import burlap.behavior.learningrate.SoftTimeInverseDecayLR;
import burlap.behavior.policy.RandomPolicy;
import burlap.behavior.policy.support.ActionProb;
import burlap.behavior.singleagent.planning.stochastic.DynamicProgramming;
import burlap.behavior.stochasticgames.JointPolicy;
import burlap.behavior.stochasticgames.agents.maql.MultiAgentQLearning;
import burlap.behavior.stochasticgames.madynamicprogramming.MultiAgentQSourceProvider;
import burlap.behavior.stochasticgames.madynamicprogramming.backupOperators.CoCoQ;
import burlap.behavior.stochasticgames.madynamicprogramming.backupOperators.MaxQ;
import burlap.behavior.stochasticgames.madynamicprogramming.policies.EGreedyJointPolicy;
import burlap.domain.singleagent.blockdude.state.BlockDudeAgent;
import burlap.mdp.core.action.Action;
import burlap.mdp.core.action.ActionType;
import burlap.mdp.core.action.UniversalActionType;
import burlap.mdp.core.state.State;
import burlap.mdp.singleagent.SADomain;
import burlap.mdp.singleagent.oo.OOSADomain;
import burlap.mdp.stochasticgames.JointAction;
import burlap.mdp.stochasticgames.agent.SGAgentType;
import burlap.mdp.stochasticgames.oo.OOSGDomain;
import burlap.statehashing.HashableStateFactory;
import burlap.statehashing.ReflectiveHashableStateFactory;
import java.util.ArrayList;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Vector;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class EGreedyJointPolicy_ESTest extends EGreedyJointPolicy_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      Vector<SGAgentType> vector0 = new Vector<SGAgentType>();
      DynamicProgramming dynamicProgramming0 = new DynamicProgramming();
      CoCoQ coCoQ0 = new CoCoQ();
      LinkedHashSet<UniversalActionType> linkedHashSet0 = new LinkedHashSet<UniversalActionType>();
      Vector<ActionType> vector1 = new Vector<ActionType>(linkedHashSet0);
      SGAgentType sGAgentType0 = new SGAgentType((String) null, vector1);
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, 116.47515, (LearningRate) null, (HashableStateFactory) null, dynamicProgramming0, coCoQ0, false, "burlap.behavior.stochasticgames.madynamicprogramming.policies.EGreedyJointPolicy", sGAgentType0);
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(multiAgentQLearning0, 0.0, (-1408));
      eGreedyJointPolicy0.setAgentTypesInJointPolicy(vector0);
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      // Undeclared exception!
      try { 
        eGreedyJointPolicy0.action(blockDudeAgent0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.behavior.stochasticgames.madynamicprogramming.policies.EGreedyJointPolicy", e);
      }
  }

  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      MaxQ maxQ0 = new MaxQ();
      OOSADomain oOSADomain0 = new OOSADomain();
      RandomPolicy randomPolicy0 = new RandomPolicy(oOSADomain0);
      List<ActionType> list0 = randomPolicy0.getSelectionActions();
      SGAgentType sGAgentType0 = new SGAgentType("", list0);
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, (-727.0609013118), 0.0, reflectiveHashableStateFactory0, (-197.35448981714873), maxQ0, true, "", sGAgentType0);
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(multiAgentQLearning0, (-197.35448981714873), (-1431));
      JointPolicy jointPolicy0 = eGreedyJointPolicy0.copy();
      assertNotSame(jointPolicy0, eGreedyJointPolicy0);
  }

  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy((MultiAgentQLearning) null, 0, 0);
      // Undeclared exception!
      try { 
        eGreedyJointPolicy0.policyDistribution(blockDudeAgent0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(116.47515);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      Vector<SGAgentType> vector0 = new Vector<SGAgentType>();
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      SGAgentType sGAgentType0 = new SGAgentType("", arrayList0);
      vector0.add(sGAgentType0);
      eGreedyJointPolicy0.setAgentTypesInJointPolicy(vector0);
      DynamicProgramming dynamicProgramming0 = new DynamicProgramming();
      SoftTimeInverseDecayLR softTimeInverseDecayLR0 = new SoftTimeInverseDecayLR(116.47515, 116.47515, (HashableStateFactory) null, true);
      CoCoQ coCoQ0 = new CoCoQ();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, (-1759), softTimeInverseDecayLR0, (HashableStateFactory) null, dynamicProgramming0, coCoQ0, true, "", sGAgentType0);
      eGreedyJointPolicy0.qSourceProvider = (MultiAgentQSourceProvider) multiAgentQLearning0;
      // Undeclared exception!
      try { 
        eGreedyJointPolicy0.actionProb((State) null, (Action) null);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Policy is undefined for the provided state
         //
         verifyException("burlap.behavior.policy.PolicyUtils", e);
      }
  }

  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(0.0);
      JointAction jointAction0 = new JointAction();
      // Undeclared exception!
      try { 
        eGreedyJointPolicy0.actionProb((State) null, jointAction0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(116.47515);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      Vector<SGAgentType> vector0 = new Vector<SGAgentType>();
      eGreedyJointPolicy0.setAgentTypesInJointPolicy(vector0);
      DynamicProgramming dynamicProgramming0 = new DynamicProgramming();
      SoftTimeInverseDecayLR softTimeInverseDecayLR0 = new SoftTimeInverseDecayLR(116.47515, 116.47515, (HashableStateFactory) null, true);
      CoCoQ coCoQ0 = new CoCoQ();
      SGAgentType sGAgentType0 = new SGAgentType("r", (List<ActionType>) null);
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, (-1759), softTimeInverseDecayLR0, (HashableStateFactory) null, dynamicProgramming0, coCoQ0, false, "r", sGAgentType0);
      eGreedyJointPolicy0.qSourceProvider = (MultiAgentQSourceProvider) multiAgentQLearning0;
      // Undeclared exception!
      try { 
        eGreedyJointPolicy0.policyDistribution((State) null);
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // org/apache/commons/lang3/builder/HashCodeBuilder
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(Double.NEGATIVE_INFINITY);
      eGreedyJointPolicy0.setQSourceProvider((MultiAgentQSourceProvider) null);
  }

  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(116.47515);
      eGreedyJointPolicy0.setTargetAgent(1);
  }

  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(116.47515);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      Vector<SGAgentType> vector0 = new Vector<SGAgentType>();
      SADomain sADomain0 = new SADomain();
      RandomPolicy randomPolicy0 = new RandomPolicy(sADomain0);
      List<ActionType> list0 = randomPolicy0.getSelectionActions();
      SGAgentType sGAgentType0 = new SGAgentType(":zP>k/q", list0);
      vector0.add(sGAgentType0);
      eGreedyJointPolicy0.setAgentTypesInJointPolicy(vector0);
      DynamicProgramming dynamicProgramming0 = new DynamicProgramming();
      SoftTimeInverseDecayLR softTimeInverseDecayLR0 = new SoftTimeInverseDecayLR((-1759), 116.47515);
      CoCoQ coCoQ0 = new CoCoQ();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, (-1759), softTimeInverseDecayLR0, (HashableStateFactory) null, dynamicProgramming0, coCoQ0, false, "vM.omYwMc|", sGAgentType0);
      eGreedyJointPolicy0.qSourceProvider = (MultiAgentQSourceProvider) multiAgentQLearning0;
      List<ActionProb> list1 = eGreedyJointPolicy0.policyDistribution((State) null);
      assertEquals(0, list1.size());
  }

  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(116.47515);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      Vector<SGAgentType> vector0 = new Vector<SGAgentType>();
      SADomain sADomain0 = new SADomain();
      RandomPolicy randomPolicy0 = new RandomPolicy(sADomain0);
      List<ActionType> list0 = randomPolicy0.getSelectionActions();
      SGAgentType sGAgentType0 = new SGAgentType(":zP>k/q", list0);
      vector0.add(sGAgentType0);
      eGreedyJointPolicy0.epsilon = (double) (-1759);
      eGreedyJointPolicy0.setAgentTypesInJointPolicy(vector0);
      DynamicProgramming dynamicProgramming0 = new DynamicProgramming();
      SoftTimeInverseDecayLR softTimeInverseDecayLR0 = new SoftTimeInverseDecayLR((-1759), 116.47515);
      CoCoQ coCoQ0 = new CoCoQ();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, (-1759), softTimeInverseDecayLR0, (HashableStateFactory) null, dynamicProgramming0, coCoQ0, false, "vM.omYwMc|", sGAgentType0);
      eGreedyJointPolicy0.qSourceProvider = (MultiAgentQSourceProvider) multiAgentQLearning0;
      // Undeclared exception!
      try { 
        eGreedyJointPolicy0.action((State) null);
        fail("Expecting exception: ArithmeticException");
      
      } catch(ArithmeticException e) {
         //
         // / by zero
         //
         verifyException("org.evosuite.runtime.Random", e);
      }
  }

  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(116.47515);
      boolean boolean0 = eGreedyJointPolicy0.definedFor((State) null);
      assertTrue(boolean0);
  }

  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      EGreedyJointPolicy eGreedyJointPolicy0 = new EGreedyJointPolicy(116.47515);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      Vector<SGAgentType> vector0 = new Vector<SGAgentType>();
      eGreedyJointPolicy0.setAgentTypesInJointPolicy(vector0);
      DynamicProgramming dynamicProgramming0 = new DynamicProgramming();
      SoftTimeInverseDecayLR softTimeInverseDecayLR0 = new SoftTimeInverseDecayLR(116.47515, 116.47515, (HashableStateFactory) null, true);
      CoCoQ coCoQ0 = new CoCoQ();
      SGAgentType sGAgentType0 = new SGAgentType("r", (List<ActionType>) null);
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, (-1759), softTimeInverseDecayLR0, (HashableStateFactory) null, dynamicProgramming0, coCoQ0, false, "r", sGAgentType0);
      eGreedyJointPolicy0.qSourceProvider = (MultiAgentQSourceProvider) multiAgentQLearning0;
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      Action action0 = eGreedyJointPolicy0.action(blockDudeAgent0);
      // Undeclared exception!
      try { 
        eGreedyJointPolicy0.actionProb(blockDudeAgent0, action0);
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // org/apache/commons/lang3/builder/HashCodeBuilder
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }
}
