/*
 * This file was automatically generated by EvoSuite
 * Sun Nov 06 01:56:52 GMT 2022
 */

package burlap.behavior.stochasticgames.madynamicprogramming.policies;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import burlap.behavior.learningrate.SoftTimeInverseDecayLR;
import burlap.behavior.singleagent.learning.tdmethods.QLearning;
import burlap.behavior.stochasticgames.JointPolicy;
import burlap.behavior.stochasticgames.agents.maql.MultiAgentQLearning;
import burlap.behavior.stochasticgames.madynamicprogramming.SGBackupOperator;
import burlap.behavior.stochasticgames.madynamicprogramming.backupOperators.CoCoQ;
import burlap.behavior.stochasticgames.madynamicprogramming.backupOperators.CorrelatedQ;
import burlap.behavior.stochasticgames.madynamicprogramming.policies.EGreedyMaxWellfare;
import burlap.behavior.stochasticgames.solvers.CorrelatedEquilibriumSolver;
import burlap.domain.singleagent.blockdude.state.BlockDudeAgent;
import burlap.mdp.auxiliary.common.NullTermination;
import burlap.mdp.core.TerminalFunction;
import burlap.mdp.core.action.Action;
import burlap.mdp.core.action.ActionType;
import burlap.mdp.core.action.UniversalActionType;
import burlap.mdp.singleagent.pomdp.PODomain;
import burlap.mdp.stochasticgames.JointAction;
import burlap.mdp.stochasticgames.SGDomain;
import burlap.mdp.stochasticgames.agent.SGAgentType;
import burlap.mdp.stochasticgames.model.JointRewardFunction;
import burlap.mdp.stochasticgames.oo.OOSGDomain;
import burlap.mdp.stochasticgames.world.World;
import burlap.statehashing.HashableStateFactory;
import burlap.statehashing.ReflectiveHashableStateFactory;
import java.util.Stack;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class EGreedyMaxWellfare_ESTest extends EGreedyMaxWellfare_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      Stack<ActionType> stack0 = new Stack<ActionType>();
      SGAgentType sGAgentType0 = new SGAgentType("v<1{", stack0);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      CoCoQ coCoQ0 = new CoCoQ();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, 0, 0, (HashableStateFactory) null, 0, coCoQ0, false, "v<1{", sGAgentType0);
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(multiAgentQLearning0, 1877.3117);
      World world0 = new World(oOSGDomain0, (JointRewardFunction) null, (TerminalFunction) null, blockDudeAgent0);
      eGreedyMaxWellfare0.setAgentsInJointPolicyFromWorld(world0);
      // Undeclared exception!
      try { 
        eGreedyMaxWellfare0.policyDistribution(blockDudeAgent0);
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // org/apache/commons/lang3/builder/HashCodeBuilder
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      Stack<ActionType> stack0 = new Stack<ActionType>();
      SGAgentType sGAgentType0 = new SGAgentType("zXC5f", stack0);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, (-2754.78321831), 0, (HashableStateFactory) null, 0, (SGBackupOperator) null, true, (String) null, sGAgentType0);
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(multiAgentQLearning0, (-1.2));
      World world0 = new World(oOSGDomain0, (JointRewardFunction) null, (TerminalFunction) null, blockDudeAgent0);
      eGreedyMaxWellfare0.setAgentsInJointPolicyFromWorld(world0);
      JointAction jointAction0 = (JointAction)eGreedyMaxWellfare0.action(blockDudeAgent0);
      assertEquals(0, jointAction0.size());
  }

  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      SGDomain sGDomain0 = new SGDomain();
      Stack<ActionType> stack0 = new Stack<ActionType>();
      SGAgentType sGAgentType0 = new SGAgentType("zXC5f", stack0);
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(sGDomain0, 0.0, 0, (HashableStateFactory) null, 1885.0, (SGBackupOperator) null, false, "angle", sGAgentType0);
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(multiAgentQLearning0, 0);
      World world0 = new World(sGDomain0, (JointRewardFunction) null, (TerminalFunction) null, blockDudeAgent0);
      eGreedyMaxWellfare0.setAgentsInJointPolicyFromWorld(world0);
      Action action0 = eGreedyMaxWellfare0.action(blockDudeAgent0);
      Action action1 = eGreedyMaxWellfare0.action(blockDudeAgent0);
      assertNotSame(action1, action0);
  }

  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare((MultiAgentQLearning) null, 0, false);
  }

  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare((MultiAgentQLearning) null, 588.8516492, true);
  }

  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(0, false);
  }

  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(1.0E-6);
  }

  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(0);
      // Undeclared exception!
      try { 
        eGreedyMaxWellfare0.policyDistribution(blockDudeAgent0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(0);
      // Undeclared exception!
      try { 
        eGreedyMaxWellfare0.action(blockDudeAgent0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(0.0);
      SGDomain sGDomain0 = new SGDomain();
      SoftTimeInverseDecayLR softTimeInverseDecayLR0 = new SoftTimeInverseDecayLR((-593.6), (-1661.91465787474));
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      PODomain pODomain0 = new PODomain();
      QLearning qLearning0 = new QLearning(pODomain0, 1903.333674865, reflectiveHashableStateFactory0, 0.0, 2962.41794603, 0);
      CorrelatedEquilibriumSolver.CorrelatedEquilibriumObjective correlatedEquilibriumSolver_CorrelatedEquilibriumObjective0 = CorrelatedEquilibriumSolver.CorrelatedEquilibriumObjective.UTILITARIAN;
      CorrelatedQ correlatedQ0 = new CorrelatedQ(correlatedEquilibriumSolver_CorrelatedEquilibriumObjective0);
      Stack<ActionType> stack0 = new Stack<ActionType>();
      SGAgentType sGAgentType0 = new SGAgentType(")Cz~<w\"Pv[", stack0);
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(sGDomain0, 0.0, softTimeInverseDecayLR0, reflectiveHashableStateFactory0, qLearning0, correlatedQ0, true, "", sGAgentType0);
      eGreedyMaxWellfare0.setQSourceProvider(multiAgentQLearning0);
      assertEquals("", multiAgentQLearning0.agentName());
  }

  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      CoCoQ coCoQ0 = new CoCoQ();
      SGDomain sGDomain0 = new SGDomain();
      UniversalActionType universalActionType0 = new UniversalActionType("");
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(sGDomain0, 0, 0, reflectiveHashableStateFactory0, 0, coCoQ0, false, "", (SGAgentType) null);
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(multiAgentQLearning0, 0);
      NullTermination nullTermination0 = new NullTermination();
      World world0 = new World(sGDomain0, (JointRewardFunction) null, nullTermination0, blockDudeAgent0);
      eGreedyMaxWellfare0.setAgentsInJointPolicyFromWorld(world0);
      // Undeclared exception!
      try { 
        eGreedyMaxWellfare0.actionProb(blockDudeAgent0, universalActionType0.action);
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // org/apache/commons/lang3/builder/HashCodeBuilder
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      CoCoQ coCoQ0 = new CoCoQ();
      SGDomain sGDomain0 = new SGDomain();
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(sGDomain0, 1266.96169993, 0, reflectiveHashableStateFactory0, 0, coCoQ0, false, "", (SGAgentType) null);
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(multiAgentQLearning0, 4419.188230269991);
      NullTermination nullTermination0 = new NullTermination();
      World world0 = new World(sGDomain0, (JointRewardFunction) null, nullTermination0, blockDudeAgent0);
      eGreedyMaxWellfare0.setAgentsInJointPolicyFromWorld(world0);
      JointAction jointAction0 = (JointAction)eGreedyMaxWellfare0.action(blockDudeAgent0);
      assertEquals(0, jointAction0.size());
  }

  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(0);
      eGreedyMaxWellfare0.setBreakTiesRandomly(false);
  }

  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      CoCoQ coCoQ0 = new CoCoQ();
      SGDomain sGDomain0 = new SGDomain();
      UniversalActionType universalActionType0 = new UniversalActionType("");
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(sGDomain0, 0, 0, reflectiveHashableStateFactory0, 0, coCoQ0, false, "", (SGAgentType) null);
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(multiAgentQLearning0, 0);
      // Undeclared exception!
      try { 
        eGreedyMaxWellfare0.actionProb(blockDudeAgent0, universalActionType0.action);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.mdp.stochasticgames.JointAction", e);
      }
  }

  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent();
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(0);
      boolean boolean0 = eGreedyMaxWellfare0.definedFor(blockDudeAgent0);
      assertTrue(boolean0);
  }

  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare((-1900.95015404867), true);
      eGreedyMaxWellfare0.setTargetAgent(0);
  }

  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      Stack<ActionType> stack0 = new Stack<ActionType>();
      SGAgentType sGAgentType0 = new SGAgentType("angle", stack0);
      OOSGDomain oOSGDomain0 = new OOSGDomain();
      MultiAgentQLearning multiAgentQLearning0 = new MultiAgentQLearning(oOSGDomain0, 0, 0, (HashableStateFactory) null, (-2259.0), (SGBackupOperator) null, true, "", sGAgentType0);
      EGreedyMaxWellfare eGreedyMaxWellfare0 = new EGreedyMaxWellfare(multiAgentQLearning0, 1.0E-6);
      JointPolicy jointPolicy0 = eGreedyMaxWellfare0.copy();
      assertNotSame(jointPolicy0, eGreedyMaxWellfare0);
  }
}
