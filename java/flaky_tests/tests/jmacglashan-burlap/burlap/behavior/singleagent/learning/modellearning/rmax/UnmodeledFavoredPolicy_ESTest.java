/*
 * This file was automatically generated by EvoSuite
 * Sun Nov 06 03:30:09 GMT 2022
 */

package burlap.behavior.singleagent.learning.modellearning.rmax;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import burlap.behavior.policy.BoltzmannQPolicy;
import burlap.behavior.policy.CachedPolicy;
import burlap.behavior.policy.EpsilonGreedy;
import burlap.behavior.policy.GreedyDeterministicQPolicy;
import burlap.behavior.policy.GreedyQPolicy;
import burlap.behavior.policy.Policy;
import burlap.behavior.policy.RandomPolicy;
import burlap.behavior.policy.support.ActionProb;
import burlap.behavior.singleagent.auxiliary.StateEnumerator;
import burlap.behavior.singleagent.learnfromdemo.RewardValueProjection;
import burlap.behavior.singleagent.learning.actorcritic.actor.BoltzmannActor;
import burlap.behavior.singleagent.learning.modellearning.KWIKModel;
import burlap.behavior.singleagent.learning.modellearning.models.TabularModel;
import burlap.behavior.singleagent.learning.modellearning.rmax.PotentialShapedRMax;
import burlap.behavior.singleagent.learning.modellearning.rmax.RMaxModel;
import burlap.behavior.singleagent.learning.modellearning.rmax.UnmodeledFavoredPolicy;
import burlap.behavior.singleagent.learning.tdmethods.QLearning;
import burlap.domain.singleagent.blockdude.state.BlockDudeAgent;
import burlap.domain.singleagent.blockdude.state.BlockDudeState;
import burlap.domain.singleagent.blocksworld.BlocksWorldBlock;
import burlap.mdp.auxiliary.common.NullTermination;
import burlap.mdp.core.action.Action;
import burlap.mdp.core.action.ActionType;
import burlap.mdp.core.action.SimpleAction;
import burlap.mdp.core.action.UniversalActionType;
import burlap.mdp.core.oo.state.generic.DeepOOState;
import burlap.mdp.core.oo.state.generic.GenericOOState;
import burlap.mdp.singleagent.SADomain;
import burlap.mdp.singleagent.common.GoalBasedRF;
import burlap.mdp.singleagent.common.NullRewardFunction;
import burlap.mdp.singleagent.model.RewardFunction;
import burlap.mdp.singleagent.oo.OOSADomain;
import burlap.mdp.singleagent.pomdp.PODomain;
import burlap.mdp.singleagent.pomdp.beliefstate.TabularBeliefState;
import burlap.statehashing.HashableStateFactory;
import burlap.statehashing.ReflectiveHashableStateFactory;
import java.util.ArrayList;
import java.util.List;
import java.util.Stack;
import java.util.Vector;
import javax.management.RuntimeErrorException;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class UnmodeledFavoredPolicy_ESTest extends UnmodeledFavoredPolicy_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      OOSADomain oOSADomain0 = new OOSADomain();
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      BoltzmannActor boltzmannActor0 = new BoltzmannActor(oOSADomain0, reflectiveHashableStateFactory0, 1423.7010974439663);
      Vector<ActionType> vector0 = new Vector<ActionType>();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(boltzmannActor0, (KWIKModel) null, vector0);
      PODomain pODomain0 = new PODomain();
      TabularBeliefState tabularBeliefState0 = new TabularBeliefState(pODomain0, (StateEnumerator) null);
      List<ActionProb> list0 = unmodeledFavoredPolicy0.policyDistribution(tabularBeliefState0);
      assertEquals(0, list0.size());
  }

  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      PODomain pODomain0 = new PODomain();
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      TabularModel tabularModel0 = new TabularModel(pODomain0, reflectiveHashableStateFactory0, 1);
      NullTermination nullTermination0 = new NullTermination();
      GoalBasedRF goalBasedRF0 = new GoalBasedRF(nullTermination0);
      RewardValueProjection rewardValueProjection0 = new RewardValueProjection(goalBasedRF0);
      BoltzmannQPolicy boltzmannQPolicy0 = new BoltzmannQPolicy(rewardValueProjection0, 800);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(boltzmannQPolicy0, tabularModel0, arrayList0);
      BlocksWorldBlock blocksWorldBlock0 = new BlocksWorldBlock();
      List<ActionProb> list0 = unmodeledFavoredPolicy0.policyDistribution(blocksWorldBlock0);
      assertFalse(list0.isEmpty());
  }

  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      PODomain pODomain0 = new PODomain();
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      TabularModel tabularModel0 = new TabularModel(pODomain0, reflectiveHashableStateFactory0, (-9));
      NullTermination nullTermination0 = new NullTermination();
      GoalBasedRF goalBasedRF0 = new GoalBasedRF(nullTermination0);
      RewardValueProjection rewardValueProjection0 = new RewardValueProjection(goalBasedRF0);
      BoltzmannQPolicy boltzmannQPolicy0 = new BoltzmannQPolicy(rewardValueProjection0, 800);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(boltzmannQPolicy0, tabularModel0, arrayList0);
      BlocksWorldBlock blocksWorldBlock0 = new BlocksWorldBlock();
      Action action0 = unmodeledFavoredPolicy0.action(blocksWorldBlock0);
      assertNull(action0);
  }

  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      RandomPolicy randomPolicy0 = new RandomPolicy(arrayList0);
      TabularModel tabularModel0 = new TabularModel((SADomain) null, (HashableStateFactory) null, 0);
      DeepOOState deepOOState0 = new DeepOOState();
      UniversalActionType universalActionType0 = new UniversalActionType("Cannot return policy distribution because source policy does not implement EnumerablePolicy");
      randomPolicy0.addAction(universalActionType0);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(randomPolicy0, tabularModel0, arrayList0);
      SimpleAction simpleAction0 = (SimpleAction)unmodeledFavoredPolicy0.action(deepOOState0);
      assertEquals("Cannot return policy distribution because source policy does not implement EnumerablePolicy", simpleAction0.getName());
  }

  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      OOSADomain oOSADomain0 = new OOSADomain();
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      QLearning qLearning0 = new QLearning(oOSADomain0, 679.5482095, reflectiveHashableStateFactory0, (-2609.894298766082), 0.0);
      EpsilonGreedy epsilonGreedy0 = new EpsilonGreedy(qLearning0, 679.5482095);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(epsilonGreedy0, (KWIKModel) null, arrayList0);
      TabularBeliefState tabularBeliefState0 = new TabularBeliefState();
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.policyDistribution(tabularBeliefState0);
        fail("Expecting exception: RuntimeErrorException");
      
      } catch(RuntimeErrorException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.behavior.singleagent.learning.tdmethods.QLearning", e);
      }
  }

  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      BlocksWorldBlock blocksWorldBlock0 = new BlocksWorldBlock();
      GreedyDeterministicQPolicy greedyDeterministicQPolicy0 = new GreedyDeterministicQPolicy();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(greedyDeterministicQPolicy0, (KWIKModel) null, arrayList0);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.policyDistribution(blocksWorldBlock0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.behavior.policy.GreedyDeterministicQPolicy", e);
      }
  }

  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NullRewardFunction nullRewardFunction0 = new NullRewardFunction();
      RewardValueProjection.RewardProjectionType rewardValueProjection_RewardProjectionType0 = RewardValueProjection.RewardProjectionType.SOURCESTATE;
      SADomain sADomain0 = new SADomain();
      RewardValueProjection rewardValueProjection0 = new RewardValueProjection(nullRewardFunction0, rewardValueProjection_RewardProjectionType0, sADomain0);
      EpsilonGreedy epsilonGreedy0 = new EpsilonGreedy(rewardValueProjection0, (-1.0));
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(epsilonGreedy0, (KWIKModel) null, arrayList0);
      BlockDudeAgent blockDudeAgent0 = new BlockDudeAgent((-4101), (-875), (-875), false);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.actionProb(blockDudeAgent0, (Action) null);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Policy is undefined for the provided state
         //
         verifyException("burlap.behavior.policy.PolicyUtils", e);
      }
  }

  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      RandomPolicy randomPolicy0 = new RandomPolicy(arrayList0);
      DeepOOState deepOOState0 = new DeepOOState();
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      CachedPolicy cachedPolicy0 = new CachedPolicy(reflectiveHashableStateFactory0, randomPolicy0);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(cachedPolicy0, (KWIKModel) null, arrayList0);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.actionProb(deepOOState0, (Action) null);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Reflective Hashable State should only be used with State objects that also already implement HashableState.
         //
         verifyException("burlap.statehashing.ReflectiveHashableStateFactory", e);
      }
  }

  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy((Policy) null, (KWIKModel) null, arrayList0);
      BlocksWorldBlock blocksWorldBlock0 = new BlocksWorldBlock();
      SimpleAction simpleAction0 = new SimpleAction("__table__");
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.actionProb(blocksWorldBlock0, simpleAction0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.behavior.singleagent.learning.modellearning.rmax.UnmodeledFavoredPolicy", e);
      }
  }

  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NullRewardFunction nullRewardFunction0 = new NullRewardFunction();
      RewardValueProjection.RewardProjectionType rewardValueProjection_RewardProjectionType0 = RewardValueProjection.RewardProjectionType.SOURCESTATE;
      SADomain sADomain0 = new SADomain();
      RewardValueProjection rewardValueProjection0 = new RewardValueProjection(nullRewardFunction0, rewardValueProjection_RewardProjectionType0, sADomain0);
      SimpleAction simpleAction0 = new SimpleAction("burlap.behavior.singleagent.learning.lspi.SARSCollector");
      GreedyQPolicy greedyQPolicy0 = new GreedyQPolicy(rewardValueProjection0);
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(greedyQPolicy0, (KWIKModel) null, arrayList0);
      GenericOOState genericOOState0 = new GenericOOState();
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.actionProb(genericOOState0, simpleAction0);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
      }
  }

  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      RandomPolicy randomPolicy0 = new RandomPolicy(arrayList0);
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      CachedPolicy cachedPolicy0 = new CachedPolicy(reflectiveHashableStateFactory0, randomPolicy0);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(cachedPolicy0, (KWIKModel) null, arrayList0);
      BlocksWorldBlock blocksWorldBlock0 = new BlocksWorldBlock();
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.action(blocksWorldBlock0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Reflective Hashable State should only be used with State objects that also already implement HashableState.
         //
         verifyException("burlap.statehashing.ReflectiveHashableStateFactory", e);
      }
  }

  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      BlocksWorldBlock blocksWorldBlock0 = new BlocksWorldBlock();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy((Policy) null, (KWIKModel) null, arrayList0);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.action(blocksWorldBlock0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("burlap.behavior.singleagent.learning.modellearning.rmax.UnmodeledFavoredPolicy", e);
      }
  }

  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      DeepOOState deepOOState0 = new DeepOOState();
      NullRewardFunction nullRewardFunction0 = new NullRewardFunction();
      RewardValueProjection.RewardProjectionType rewardValueProjection_RewardProjectionType0 = RewardValueProjection.RewardProjectionType.SOURCESTATE;
      OOSADomain oOSADomain0 = new OOSADomain();
      RewardValueProjection rewardValueProjection0 = new RewardValueProjection(nullRewardFunction0, rewardValueProjection_RewardProjectionType0, oOSADomain0);
      GreedyQPolicy greedyQPolicy0 = new GreedyQPolicy(rewardValueProjection0);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(greedyQPolicy0, (KWIKModel) null, arrayList0);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.action(deepOOState0);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
      }
  }

  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      RewardValueProjection.RewardProjectionType rewardValueProjection_RewardProjectionType0 = RewardValueProjection.RewardProjectionType.DESTINATIONSTATE;
      OOSADomain oOSADomain0 = new OOSADomain();
      RewardValueProjection rewardValueProjection0 = new RewardValueProjection((RewardFunction) null, rewardValueProjection_RewardProjectionType0, oOSADomain0);
      EpsilonGreedy epsilonGreedy0 = new EpsilonGreedy(rewardValueProjection0, 124.0);
      ReflectiveHashableStateFactory reflectiveHashableStateFactory0 = new ReflectiveHashableStateFactory();
      TabularModel tabularModel0 = new TabularModel(oOSADomain0, reflectiveHashableStateFactory0, 2490);
      PotentialShapedRMax.RMaxPotential potentialShapedRMax_RMaxPotential0 = new PotentialShapedRMax.RMaxPotential(1919.9351);
      Stack<ActionType> stack0 = new Stack<ActionType>();
      RMaxModel rMaxModel0 = new RMaxModel(tabularModel0, potentialShapedRMax_RMaxPotential0, 1919.9351, stack0);
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(epsilonGreedy0, rMaxModel0, stack0);
      TabularBeliefState tabularBeliefState0 = new TabularBeliefState();
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.action(tabularBeliefState0);
        fail("Expecting exception: ArithmeticException");
      
      } catch(ArithmeticException e) {
         //
         // / by zero
         //
         verifyException("org.evosuite.runtime.Random", e);
      }
  }

  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      BlockDudeState blockDudeState0 = new BlockDudeState();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy((Policy) null, (KWIKModel) null, arrayList0);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.policyDistribution(blockDudeState0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Cannot return policy distribution because source policy does not implement EnumerablePolicy
         //
         verifyException("burlap.behavior.singleagent.learning.modellearning.rmax.UnmodeledFavoredPolicy", e);
      }
  }

  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      RandomPolicy randomPolicy0 = new RandomPolicy(arrayList0);
      TabularModel tabularModel0 = new TabularModel((SADomain) null, (HashableStateFactory) null, 0);
      DeepOOState deepOOState0 = new DeepOOState();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(randomPolicy0, tabularModel0, arrayList0);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.policyDistribution(deepOOState0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Policy is undefined for the provided state
         //
         verifyException("burlap.behavior.policy.RandomPolicy", e);
      }
  }

  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      RandomPolicy randomPolicy0 = new RandomPolicy(arrayList0);
      TabularModel tabularModel0 = new TabularModel((SADomain) null, (HashableStateFactory) null, 0);
      DeepOOState deepOOState0 = new DeepOOState();
      SimpleAction simpleAction0 = new SimpleAction("D");
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(randomPolicy0, tabularModel0, arrayList0);
      double double0 = unmodeledFavoredPolicy0.actionProb(deepOOState0, simpleAction0);
      assertEquals(Double.POSITIVE_INFINITY, double0, 0.01);
  }

  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      RandomPolicy randomPolicy0 = new RandomPolicy(arrayList0);
      TabularModel tabularModel0 = new TabularModel((SADomain) null, (HashableStateFactory) null, 0);
      DeepOOState deepOOState0 = new DeepOOState();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(randomPolicy0, tabularModel0, arrayList0);
      // Undeclared exception!
      try { 
        unmodeledFavoredPolicy0.action(deepOOState0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Policy is undefined for the provided state
         //
         verifyException("burlap.behavior.policy.RandomPolicy", e);
      }
  }

  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      ArrayList<ActionType> arrayList0 = new ArrayList<ActionType>();
      RandomPolicy randomPolicy0 = new RandomPolicy(arrayList0);
      TabularModel tabularModel0 = new TabularModel((SADomain) null, (HashableStateFactory) null, 0);
      DeepOOState deepOOState0 = new DeepOOState();
      UnmodeledFavoredPolicy unmodeledFavoredPolicy0 = new UnmodeledFavoredPolicy(randomPolicy0, tabularModel0, arrayList0);
      boolean boolean0 = unmodeledFavoredPolicy0.definedFor(deepOOState0);
      assertTrue(boolean0);
  }
}
