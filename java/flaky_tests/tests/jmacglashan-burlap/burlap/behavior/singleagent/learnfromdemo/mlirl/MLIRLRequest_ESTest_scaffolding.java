/**
 * Scaffolding file used to store all the setups needed to run 
 * tests automatically generated by EvoSuite
 * Sun Nov 06 02:49:41 GMT 2022
 */

package burlap.behavior.singleagent.learnfromdemo.mlirl;

import org.evosuite.runtime.annotation.EvoSuiteClassExclude;
import org.junit.BeforeClass;
import org.junit.Before;
import org.junit.After;
import org.junit.AfterClass;
import org.evosuite.runtime.sandbox.Sandbox;
import org.evosuite.runtime.sandbox.Sandbox.SandboxMode;

@EvoSuiteClassExclude
public class MLIRLRequest_ESTest_scaffolding {

  @org.junit.Rule
  public org.evosuite.runtime.vnet.NonFunctionalRequirementRule nfr = new org.evosuite.runtime.vnet.NonFunctionalRequirementRule();

  private static final java.util.Properties defaultProperties = (java.util.Properties) java.lang.System.getProperties().clone(); 

  private org.evosuite.runtime.thread.ThreadStopper threadStopper =  new org.evosuite.runtime.thread.ThreadStopper (org.evosuite.runtime.thread.KillSwitchHandler.getInstance(), 3000);


  @BeforeClass
  public static void initEvoSuiteFramework() { 
    org.evosuite.runtime.RuntimeSettings.className = "burlap.behavior.singleagent.learnfromdemo.mlirl.MLIRLRequest"; 
    org.evosuite.runtime.GuiSupport.initialize(); 
    org.evosuite.runtime.RuntimeSettings.maxNumberOfThreads = 100; 
    org.evosuite.runtime.RuntimeSettings.maxNumberOfIterationsPerLoop = 10000; 
    org.evosuite.runtime.RuntimeSettings.mockSystemIn = true; 
    org.evosuite.runtime.RuntimeSettings.sandboxMode = org.evosuite.runtime.sandbox.Sandbox.SandboxMode.RECOMMENDED; 
    org.evosuite.runtime.sandbox.Sandbox.initializeSecurityManagerForSUT(); 
    org.evosuite.runtime.classhandling.JDKClassResetter.init();
    setSystemProperties();
    initializeClasses();
    org.evosuite.runtime.Runtime.getInstance().resetRuntime(); 
  } 

  @AfterClass
  public static void clearEvoSuiteFramework(){ 
    Sandbox.resetDefaultSecurityManager(); 
    java.lang.System.setProperties((java.util.Properties) defaultProperties.clone()); 
  } 

  @Before
  public void initTestCase(){ 
    threadStopper.storeCurrentThreads();
    threadStopper.startRecordingTime();
    org.evosuite.runtime.jvm.ShutdownHookHandler.getInstance().initHandler(); 
    org.evosuite.runtime.sandbox.Sandbox.goingToExecuteSUTCode(); 
    setSystemProperties(); 
    org.evosuite.runtime.GuiSupport.setHeadless(); 
    org.evosuite.runtime.Runtime.getInstance().resetRuntime(); 
    org.evosuite.runtime.agent.InstrumentingAgent.activate(); 
  } 

  @After
  public void doneWithTestCase(){ 
    threadStopper.killAndJoinClientThreads();
    org.evosuite.runtime.jvm.ShutdownHookHandler.getInstance().safeExecuteAddedHooks(); 
    org.evosuite.runtime.classhandling.JDKClassResetter.reset(); 
    resetClasses(); 
    org.evosuite.runtime.sandbox.Sandbox.doneWithExecutingSUTCode(); 
    org.evosuite.runtime.agent.InstrumentingAgent.deactivate(); 
    org.evosuite.runtime.GuiSupport.restoreHeadlessMode(); 
  } 

  public static void setSystemProperties() {
 
    java.lang.System.setProperties((java.util.Properties) defaultProperties.clone()); 
    java.lang.System.setProperty("user.dir", "/home/user"); 
    java.lang.System.setProperty("java.io.tmpdir", "/tmp"); 
  }

  private static void initializeClasses() {
    org.evosuite.runtime.classhandling.ClassStateSupport.initializeClasses(MLIRLRequest_ESTest_scaffolding.class.getClassLoader() ,
      "burlap.behavior.functionapproximation.FunctionGradient",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.support.DifferentiableRF",
      "burlap.behavior.singleagent.Episode",
      "burlap.behavior.policy.Policy",
      "burlap.behavior.singleagent.learnfromdemo.IRLRequest",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.dpoperator.DifferentiableSoftmaxOperator",
      "burlap.mdp.stochasticgames.oo.OOSGDomain",
      "burlap.mdp.core.oo.propositional.PropositionalFunction",
      "burlap.mdp.stochasticgames.SGDomain",
      "burlap.behavior.valuefunction.ValueFunction",
      "burlap.behavior.singleagent.MDPSolver",
      "burlap.mdp.singleagent.pomdp.PODomain",
      "burlap.behavior.functionapproximation.dense.PFFeatures",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.commonrfs.LinearStateDifferentiableRF",
      "burlap.behavior.valuefunction.ConstantValueFunction",
      "burlap.mdp.core.action.Action",
      "burlap.statehashing.ReflectiveHashableStateFactory",
      "burlap.behavior.singleagent.planning.stochastic.dpoperator.SoftmaxOperator",
      "burlap.mdp.core.oo.state.OOState",
      "burlap.behavior.policy.SolverDerivedPolicy",
      "burlap.mdp.singleagent.environment.EnvironmentOutcome",
      "burlap.mdp.core.oo.state.OOStateUtilities",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.diffvinit.DifferentiableVInit",
      "burlap.mdp.singleagent.oo.OOSADomain",
      "burlap.behavior.singleagent.learnfromdemo.CustomRewardModel",
      "burlap.mdp.singleagent.model.RewardFunction",
      "burlap.mdp.core.state.State",
      "burlap.mdp.core.action.ActionType",
      "burlap.mdp.stochasticgames.model.JointModel",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.MLIRLRequest",
      "burlap.statehashing.HashableState",
      "burlap.mdp.core.oo.state.exceptions.UnknownObjectException",
      "burlap.behavior.policy.EnumerablePolicy",
      "burlap.mdp.core.oo.state.ObjectInstance",
      "burlap.debugtools.RandomFactory",
      "burlap.behavior.functionapproximation.dense.DenseStateFeatures",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.DifferentiableSparseSampling",
      "burlap.behavior.singleagent.planning.Planner",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.dpoperator.DifferentiableDPOperator",
      "burlap.mdp.singleagent.pomdp.observations.ObservationFunction",
      "burlap.behavior.singleagent.auxiliary.StateEnumerator",
      "burlap.behavior.valuefunction.QProvider",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.DifferentiableVI",
      "burlap.mdp.singleagent.SADomain",
      "burlap.behavior.functionapproximation.dense.NumericVariableFeatures",
      "burlap.mdp.singleagent.pomdp.beliefstate.TabularBeliefState",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.diffvinit.VanillaDiffVinit",
      "burlap.mdp.core.Domain",
      "burlap.behavior.singleagent.planning.stochastic.DynamicProgramming",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.support.DifferentiableValueFunction",
      "burlap.mdp.core.oo.state.MutableOOState",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.support.DifferentiableQFunction",
      "burlap.behavior.functionapproximation.ParametricFunction",
      "burlap.statehashing.HashableStateFactory",
      "burlap.mdp.core.state.MutableState",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.DifferentiableDP",
      "burlap.mdp.singleagent.pomdp.beliefstate.DenseBeliefVector",
      "burlap.behavior.policy.GreedyQPolicy",
      "burlap.behavior.policy.BoltzmannQPolicy",
      "burlap.behavior.singleagent.planning.stochastic.policyiteration.PolicyIteration",
      "burlap.behavior.singleagent.planning.stochastic.dpoperator.DPOperator",
      "burlap.behavior.singleagent.MDPSolverInterface",
      "burlap.mdp.singleagent.pomdp.beliefstate.EnumerableBeliefState",
      "burlap.mdp.core.oo.state.generic.DeepOOState",
      "burlap.mdp.core.oo.OODomain",
      "burlap.mdp.singleagent.pomdp.beliefstate.BeliefState",
      "burlap.mdp.core.oo.state.generic.GenericOOState",
      "burlap.behavior.valuefunction.QFunction",
      "burlap.behavior.functionapproximation.dense.ConcatenatedObjectFeatures",
      "burlap.behavior.singleagent.planning.stochastic.dpoperator.BellmanOperator",
      "burlap.mdp.singleagent.model.SampleModel",
      "burlap.mdp.singleagent.model.FullModel"
    );
  } 

  private static void resetClasses() {
    org.evosuite.runtime.classhandling.ClassResetter.getInstance().setClassLoader(MLIRLRequest_ESTest_scaffolding.class.getClassLoader()); 

    org.evosuite.runtime.classhandling.ClassStateSupport.resetClasses(
      "burlap.behavior.singleagent.learnfromdemo.IRLRequest",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.MLIRLRequest",
      "burlap.mdp.singleagent.SADomain",
      "burlap.mdp.singleagent.oo.OOSADomain",
      "burlap.mdp.core.oo.propositional.PropositionalFunction",
      "burlap.mdp.singleagent.pomdp.PODomain",
      "burlap.statehashing.ReflectiveHashableStateFactory",
      "burlap.behavior.singleagent.MDPSolver",
      "burlap.behavior.singleagent.pomdp.wrappedmdpalgs.BeliefSparseSampling",
      "burlap.mdp.singleagent.pomdp.BeliefMDPGenerator",
      "burlap.mdp.singleagent.pomdp.BeliefMDPGenerator$BeliefModel",
      "burlap.mdp.singleagent.pomdp.beliefstate.TabularBeliefUpdate",
      "burlap.mdp.stochasticgames.SGDomain",
      "burlap.mdp.stochasticgames.oo.OOSGDomain",
      "burlap.behavior.singleagent.learning.actorcritic.actor.BoltzmannActor",
      "burlap.behavior.learningrate.ConstantLR",
      "burlap.behavior.singleagent.learning.actorcritic.critics.TDLambda",
      "burlap.behavior.valuefunction.ConstantValueFunction",
      "burlap.behavior.singleagent.learning.actorcritic.ActorCritic",
      "burlap.behavior.functionapproximation.dense.PFFeatures",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.commonrfs.LinearStateDifferentiableRF",
      "burlap.behavior.singleagent.planning.stochastic.DynamicProgramming",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.DifferentiableDP",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.DifferentiableVI",
      "burlap.behavior.singleagent.planning.stochastic.dpoperator.BellmanOperator",
      "burlap.behavior.singleagent.planning.stochastic.dpoperator.SoftmaxOperator",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.dpoperator.DifferentiableSoftmaxOperator",
      "burlap.behavior.singleagent.planning.stochastic.policyiteration.PolicyIteration",
      "burlap.behavior.policy.GreedyQPolicy",
      "burlap.debugtools.RandomFactory",
      "burlap.behavior.singleagent.learning.actorcritic.critics.TimeIndexedTDLambda",
      "burlap.behavior.functionapproximation.dense.NormalizedVariableFeatures",
      "burlap.domain.singleagent.blockdude.state.BlockDudeState",
      "burlap.mdp.core.oo.state.generic.GenericOOState",
      "burlap.mdp.core.oo.state.generic.DeepOOState",
      "burlap.mdp.stochasticgames.JointAction",
      "burlap.mdp.core.action.UniversalActionType",
      "burlap.behavior.singleagent.Episode",
      "burlap.behavior.singleagent.Episode$1",
      "burlap.datastructures.AlphanumericSorting",
      "burlap.domain.singleagent.blocksworld.BlocksWorldBlock",
      "burlap.mdp.singleagent.common.GoalBasedRF",
      "burlap.mdp.auxiliary.stateconditiontest.TFGoalCondition",
      "burlap.behavior.singleagent.learnfromdemo.RewardValueProjection",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.commonrfs.LinearStateActionDifferentiableRF",
      "burlap.mdp.singleagent.pomdp.beliefstate.TabularBeliefState",
      "burlap.mdp.core.action.SimpleAction",
      "burlap.behavior.functionapproximation.dense.SparseToDenseFeatures",
      "burlap.domain.singleagent.mountaincar.MountainCar$MCPhysicsParams",
      "burlap.domain.singleagent.mountaincar.MountainCar$MCModel",
      "burlap.domain.singleagent.blockdude.state.BlockDudeMap",
      "burlap.domain.singleagent.blockdude.state.BlockDudeCell",
      "burlap.behavior.policy.PolicyUtils",
      "burlap.behavior.functionapproximation.dense.NumericVariableFeatures",
      "burlap.behavior.policy.BoltzmannQPolicy",
      "burlap.behavior.policy.RandomPolicy",
      "burlap.domain.singleagent.blockdude.state.BlockDudeAgent",
      "burlap.mdp.auxiliary.common.GoalConditionTF",
      "burlap.mdp.core.state.NullState",
      "burlap.behavior.singleagent.planning.stochastic.rtdp.BoundedRTDP",
      "burlap.behavior.singleagent.planning.stochastic.rtdp.BoundedRTDP$StateSelectionMode",
      "burlap.behavior.functionapproximation.dense.ConcatenatedObjectFeatures",
      "burlap.behavior.singleagent.planning.stochastic.sparsesampling.SparseSampling",
      "burlap.behavior.singleagent.learning.tdmethods.QLearning",
      "burlap.behavior.policy.EpsilonGreedy",
      "burlap.behavior.singleagent.learning.tdmethods.SarsaLam",
      "burlap.domain.singleagent.gridworld.GridWorldDomain$GridWorldModel",
      "burlap.domain.singleagent.lunarlander.LunarLanderDomain$LLPhysicsParams",
      "burlap.domain.singleagent.lunarlander.LunarLanderModel",
      "burlap.behavior.singleagent.pomdp.qmdp.QMDP",
      "burlap.behavior.singleagent.planning.stochastic.valueiteration.ValueIteration",
      "burlap.behavior.policy.GreedyDeterministicQPolicy",
      "burlap.mdp.auxiliary.common.RandomStartStateGenerator",
      "burlap.statehashing.simple.SimpleHashableStateFactory",
      "burlap.behavior.singleagent.auxiliary.StateReachability",
      "burlap.behavior.singleagent.planning.stochastic.rtdp.RTDP",
      "burlap.mdp.core.action.ActionUtils",
      "burlap.mdp.singleagent.environment.SimulatedEnvironment",
      "burlap.mdp.auxiliary.common.ConstantStateGenerator",
      "burlap.mdp.auxiliary.common.SinglePFTF",
      "burlap.behavior.singleagent.auxiliary.StateEnumerator",
      "burlap.mdp.singleagent.common.UniformCostRF",
      "burlap.behavior.policy.CachedPolicy",
      "burlap.domain.singleagent.pomdp.tiger.TigerObservations",
      "burlap.domain.singleagent.pomdp.tiger.TigerObservation",
      "burlap.behavior.singleagent.planning.vfa.fittedvi.FittedVI",
      "burlap.behavior.singleagent.planning.vfa.fittedvi.FittedVI$VFAVInit",
      "burlap.mdp.singleagent.model.DelegatedModel",
      "burlap.mdp.stochasticgames.common.StaticRepeatedGameModel",
      "burlap.mdp.stochasticgames.model.FullJointModel$Helper",
      "burlap.mdp.core.StateTransitionProb",
      "burlap.behavior.functionapproximation.sparse.tilecoding.TileCodingFeatures",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.DifferentiableSparseSampling",
      "burlap.mdp.core.state.vardomain.VariableDomain",
      "burlap.mdp.singleagent.common.SingleGoalPFRF",
      "burlap.mdp.auxiliary.stateconditiontest.SinglePFSCT",
      "burlap.mdp.core.oo.state.OOStateUtilities",
      "burlap.behavior.singleagent.learnfromdemo.mlirl.differentiableplanners.diffvinit.VanillaDiffVinit",
      "burlap.behavior.singleagent.learnfromdemo.CustomRewardModel",
      "burlap.debugtools.DPrint",
      "burlap.visualizer.StateRenderLayer",
      "burlap.domain.singleagent.frostbite.FrostbiteModel",
      "burlap.mdp.auxiliary.common.NullTermination",
      "burlap.mdp.singleagent.model.FactoredModel",
      "burlap.domain.singleagent.mountaincar.MCState",
      "burlap.domain.singleagent.graphdefined.GraphDefinedDomain$GraphStateModel",
      "burlap.behavior.learningrate.ExponentialDecayLR",
      "burlap.behavior.singleagent.planning.stochastic.valueiteration.PrioritizedSweeping",
      "burlap.datastructures.HashIndexedHeap",
      "burlap.behavior.singleagent.planning.stochastic.valueiteration.PrioritizedSweeping$BPTRNodeComparator",
      "burlap.domain.singleagent.blockdude.BlockDudeModel",
      "burlap.behavior.policy.support.PolicyUndefinedException",
      "burlap.domain.singleagent.gridworld.GridWorldDomain",
      "burlap.domain.singleagent.gridworld.GridWorldDomain$AtLocationPF",
      "burlap.domain.singleagent.gridworld.GridWorldDomain$WallToPF",
      "burlap.mdp.core.oo.OODomain$Helper",
      "burlap.domain.singleagent.gridworld.state.GridWorldState",
      "burlap.domain.singleagent.gridworld.state.GridAgent",
      "burlap.mdp.singleagent.environment.EnvironmentOutcome",
      "burlap.domain.singleagent.blocksworld.BWModel",
      "burlap.behavior.singleagent.learning.actorcritic.actor.BoltzmannActor$PolicyNode",
      "burlap.datastructures.BoltzmannDistribution",
      "burlap.mdp.core.state.StateUtilities",
      "burlap.behavior.singleagent.planning.stochastic.sparsesampling.SparseSampling$HashedHeightState",
      "burlap.behavior.singleagent.planning.stochastic.sparsesampling.SparseSampling$StateNode",
      "burlap.mdp.singleagent.common.NullRewardFunction",
      "burlap.visualizer.MultiLayerRenderer",
      "burlap.visualizer.Visualizer",
      "burlap.behavior.singleagent.learnfromdemo.RewardValueProjection$CustomRewardNoTermModel",
      "burlap.behavior.functionapproximation.FunctionGradient$SparseGradient",
      "burlap.behavior.functionapproximation.sparse.tilecoding.Tiling",
      "burlap.behavior.singleagent.learning.tdmethods.QLearningStateNode",
      "burlap.behavior.singleagent.planning.stochastic.valueiteration.PrioritizedSweeping$BPTRNode",
      "burlap.behavior.valuefunction.QValue",
      "burlap.mdp.core.oo.state.OOVariableKey",
      "burlap.statehashing.WrappedHashableState",
      "burlap.statehashing.simple.IDSimpleHashableState",
      "burlap.statehashing.simple.IISimpleHashableState",
      "burlap.statehashing.discretized.DiscConfig",
      "burlap.statehashing.discretized.IDDiscHashableState",
      "burlap.behavior.learningrate.SoftTimeInverseDecayLR"
    );
  }
}
