# Automatically generated by Pynguin.
import pytest
import wrktoolbox.benchmarks as module_0
import yaml as module_1
import base64 as module_2

@pytest.mark.xfail(strict=True)
def test_case_0():
    str_0 = 'x/D7M~6k'
    bool_0 = False
    module_0.BenchmarkConfig(str_0, concurrency=bool_0, responses_per_second=str_0, test_id=str_0)

@pytest.mark.xfail(strict=True)
def test_case_1():
    str_0 = 'j]yfU!)9m-\tshdhN3LjS'
    module_0.BenchmarkConfig(str_0, duration=str_0)

def test_case_2():
    str_0 = 'a0F/@9`\x0bM$'
    int_0 = 3936
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, int_0, script=str_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'

def test_case_3():
    str_0 = 'a5F59`+\x0bnM$'
    none_type_0 = None
    none_type_1 = None
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, responses_per_second=none_type_0, latency_statistics=none_type_1)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    none_type_2 = None
    int_0 = 0
    benchmark_config_1 = module_0.BenchmarkConfig(str_0, script=int_0, app_variant=int_0)
    assert f'{type(benchmark_config_1).__module__}.{type(benchmark_config_1).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    process_benchmark_exception_0 = module_0.ProcessBenchmarkException(none_type_2, int_0)
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == 'wrk a5F59`+\x0bnM$ -c 10 -t 48 -d 20 --timeout 20'
    with pytest.raises(TypeError):
        var_0.__new__(benchmark_config_1, none_type_2, str_0, benchmark_config_0)

@pytest.mark.xfail(strict=True)
def test_case_4():
    none_type_0 = None
    var_0 = module_0.handle_plugins(none_type_0)
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    benchmark_config_0 = module_0.BenchmarkConfig(var_0, responses_per_second=none_type_0, latency_statistics=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = benchmark_config_0.get_cmd()
    str_0 = 'pFwE%Gj(\tE'
    module_0.BenchmarkSuite(var_1, str_0, var_0)

def test_case_5():
    none_type_0 = None
    bool_0 = True
    str_0 = 'g=W!$dWW^SudnivY'
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_0, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == 'wrk2 g=W!$dWW^SudnivY -c 1 -t 48 -d 20 --timeout 20 -R1'

def test_case_6():
    none_type_0 = None
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'

def test_case_7():
    host_data_0 = module_0.HostData()
    assert host_data_0.cpu_count == 48
    assert host_data_0.env == {'PYNGUIN_DANGER_AWARE': '', 'PYTHONUNBUFFERED': '1', 'HOSTNAME': '493e3850223b', 'PYTHON_VERSION': '3.10.6', 'PYTHONHASHSEED': '0', 'PWD': '/pynguin', 'PYTHON_SETUPTOOLS_VERSION': '63.2.0', 'container': 'podman', 'HOME': '/root', 'LANG': 'C.UTF-8', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'TERM': 'xterm', 'SHLVL': '1', 'PYNGUIN_VERSION': '0.27.0', 'PYTHON_PIP_VERSION': '22.2.1', 'PYTHONDONTWRITEBYTECODE': '1', 'PYTHON_GET_PIP_SHA256': '5aefe6ade911d997af080b315ebcb7f882212d070465df544e1175ac2be519b4', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/5eaac1050023df1f5c98b173b248c260023f2278/public/get-pip.py', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', '_': '/usr/local/bin/pynguin'}

def test_case_8():
    goal_exception_0 = module_0.GoalException()

def test_case_9():
    str_0 = 'module'
    int_0 = 1620
    process_benchmark_exception_0 = module_0.ProcessBenchmarkException(str_0, int_0)

def test_case_10():
    missing_dependency_exception_0 = module_0.MissingDependencyException()

def test_case_11():
    str_0 = 'j|V\x0brPC\x0b;b'
    benchmark_plugin_exception_0 = module_0.BenchmarkPluginException(str_0)
    goal_exception_0 = module_0.GoalException()

def test_case_12():
    bool_0 = False
    performance_goal_result_0 = module_0.PerformanceGoalResult(bool_0, bool_0)
    assert f'{type(performance_goal_result_0).__module__}.{type(performance_goal_result_0).__qualname__}' == 'wrktoolbox.benchmarks.PerformanceGoalResult'
    assert performance_goal_result_0.success is False
    assert performance_goal_result_0.goal is False
    assert performance_goal_result_0.error is None

@pytest.mark.xfail(strict=True)
def test_case_13():
    none_type_0 = None
    var_0 = module_0.handle_plugins(none_type_0)
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    bool_0 = True
    int_0 = 1944
    module_0.BenchmarkConfig(benchmark_plugin_0, bool_0, duration=int_0, script=none_type_0, goals=none_type_0)

@pytest.mark.xfail(strict=True)
def test_case_14():
    benchmark_exception_0 = module_0.BenchmarkException()
    benchmark_plugin_0 = module_0.BenchmarkPlugin(benchmark_exception_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert f'{type(benchmark_plugin_0.module).__module__}.{type(benchmark_plugin_0.module).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkException'
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    benchmark_plugin_0.to_dict()

@pytest.mark.xfail(strict=True)
def test_case_15():
    str_0 = '\\)>5n\rxa/5s?a('
    benchmark_plugin_exception_0 = module_0.BenchmarkPluginException(str_0)
    bool_0 = False
    str_1 = ''
    process_benchmark_exception_0 = module_0.ProcessBenchmarkException(str_1, bool_0)
    host_data_0 = module_0.HostData(bool_0)
    assert host_data_0.cpu_count is False
    assert host_data_0.env == {'PYNGUIN_DANGER_AWARE': '', 'PYTHONUNBUFFERED': '1', 'HOSTNAME': '493e3850223b', 'PYTHON_VERSION': '3.10.6', 'PYTHONHASHSEED': '0', 'PWD': '/pynguin', 'PYTHON_SETUPTOOLS_VERSION': '63.2.0', 'container': 'podman', 'HOME': '/root', 'LANG': 'C.UTF-8', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'TERM': 'xterm', 'SHLVL': '1', 'PYNGUIN_VERSION': '0.27.0', 'PYTHON_PIP_VERSION': '22.2.1', 'PYTHONDONTWRITEBYTECODE': '1', 'PYTHON_GET_PIP_SHA256': '5aefe6ade911d997af080b315ebcb7f882212d070465df544e1175ac2be519b4', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/5eaac1050023df1f5c98b173b248c260023f2278/public/get-pip.py', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', '_': '/usr/local/bin/pynguin'}
    var_0 = host_data_0.to_dict()
    var_0.getstate()

@pytest.mark.xfail(strict=True)
def test_case_16():
    str_0 = 'a5F59`\x0bM$'
    none_type_0 = None
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    str_1 = "'"
    bool_0 = False
    benchmark_config_0 = module_0.BenchmarkConfig(str_1, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_0, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    bool_1 = True
    bool_2 = True
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == "wrk ' -c 0 -t 48 -d 20 --timeout 20"
    str_2 = '6Cq]V9iaW>@_2ztB'
    module_0.BenchmarkConfig(str_2, bool_1, bool_2, app_variant=str_0, responses_per_second=bool_0, goals=none_type_0)

def test_case_17():
    none_type_0 = None
    str_0 = 'JNt 3'
    bool_0 = True
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_0, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    benchmark_0 = module_0.Benchmark(benchmark_config_0)
    assert f'{type(benchmark_0).__module__}.{type(benchmark_0).__qualname__}' == 'wrktoolbox.benchmarks.Benchmark'
    with pytest.raises(module_0.MissingDependencyException):
        benchmark_0.run(benchmark_0)

def test_case_18():
    bool_0 = False
    performance_goal_result_0 = module_0.PerformanceGoalResult(bool_0, bool_0)
    assert f'{type(performance_goal_result_0).__module__}.{type(performance_goal_result_0).__qualname__}' == 'wrktoolbox.benchmarks.PerformanceGoalResult'
    assert performance_goal_result_0.success is False
    assert performance_goal_result_0.goal is False
    assert performance_goal_result_0.error is None
    var_0 = performance_goal_result_0.to_dict()

@pytest.mark.xfail(strict=True)
def test_case_19():
    benchmark_exception_0 = module_0.BenchmarkException()
    var_0 = module_1.full_load_all(benchmark_exception_0)
    var_1 = var_0.__eq__(var_0)
    str_0 = '%u'
    none_type_0 = None
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, headers=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_2 = benchmark_config_0.__eq__(benchmark_config_0)
    assert var_2 is True
    bool_0 = True
    int_0 = -268
    module_0.BenchmarkConfig(str_0, duration=bool_0, timeout=int_0, latency_statistics=none_type_0, test_id=str_0)

def test_case_20():
    none_type_0 = None
    str_0 = "'"
    bool_0 = True
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_0, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    benchmark_0 = module_0.Benchmark(benchmark_config_0)
    assert f'{type(benchmark_0).__module__}.{type(benchmark_0).__qualname__}' == 'wrktoolbox.benchmarks.Benchmark'
    benchmark_config_1 = module_0.BenchmarkConfig(benchmark_config_0, bool_0, bool_0, app_variant=benchmark_config_0, responses_per_second=bool_0, goals=none_type_0)
    assert f'{type(benchmark_config_1).__module__}.{type(benchmark_config_1).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    with pytest.raises(module_0.ProcessBenchmarkException):
        benchmark_0.run(benchmark_0)

def test_case_21():
    str_0 = 'a5F59`+\x0bnM$'
    none_type_0 = None
    int_0 = 0
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, script=int_0, app_variant=int_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    process_benchmark_exception_0 = module_0.ProcessBenchmarkException(none_type_0, int_0)
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == 'wrk a5F59`+\x0bnM$ -c 10 -t 48 -d 20 --timeout 20 --latency -s 0'
    var_1 = var_0.islower()
    var_2 = module_0.handle_plugins(process_benchmark_exception_0)
    with pytest.raises(TypeError):
        var_0.__new__(benchmark_config_0, var_1, str_0, var_1)

def test_case_22():
    none_type_0 = None
    str_0 = "'"
    bool_0 = True
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_0, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    benchmark_0 = module_0.Benchmark(benchmark_config_0)
    assert f'{type(benchmark_0).__module__}.{type(benchmark_0).__qualname__}' == 'wrktoolbox.benchmarks.Benchmark'
    benchmark_config_1 = module_0.BenchmarkConfig(benchmark_config_0, bool_0, bool_0, app_variant=benchmark_config_0, responses_per_second=bool_0, goals=none_type_0)
    assert f'{type(benchmark_config_1).__module__}.{type(benchmark_config_1).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_0 = benchmark_config_1.get_cmd()
    assert var_0 == "wrk2 <BenchmarkConfig '> -c 1 -t 1 -d 20 --timeout 20 --latency -R1"
    with pytest.raises(module_0.ProcessBenchmarkException):
        benchmark_0.run(benchmark_0)

@pytest.mark.xfail(strict=True)
def test_case_23():
    str_0 = 'a5F59`+\x0bnM$'
    module_0.BenchmarkPlugin(str_0)

@pytest.mark.xfail(strict=True)
def test_case_24():
    none_type_0 = None
    var_0 = module_0.handle_plugins(none_type_0)
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    benchmark_config_0 = module_0.BenchmarkConfig(var_0, responses_per_second=none_type_0, latency_statistics=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = benchmark_config_0.__repr__()
    var_2 = benchmark_config_0.get_cmd()
    module_0.BenchmarkSuite(var_2, var_1, var_0)

def test_case_25():
    str_0 = 'a5F59`\x0bM$'
    none_type_0 = None
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    str_1 = "'"
    bool_0 = True
    benchmark_config_0 = module_0.BenchmarkConfig(str_1, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_0, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    bool_1 = True
    bool_2 = True
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == "wrk2 ' -c 1 -t 48 -d 20 --timeout 20 -R1"
    benchmark_0 = module_0.Benchmark(benchmark_config_0)
    assert f'{type(benchmark_0).__module__}.{type(benchmark_0).__qualname__}' == 'wrktoolbox.benchmarks.Benchmark'
    var_1 = benchmark_config_0.to_dict()
    str_2 = '6Cq]V9iaW>@_2ztB'
    benchmark_config_1 = module_0.BenchmarkConfig(str_2, bool_1, bool_2, app_variant=str_0, responses_per_second=bool_0, goals=none_type_0)
    assert f'{type(benchmark_config_1).__module__}.{type(benchmark_config_1).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_2 = benchmark_config_0.__repr__()
    assert var_2 == "<BenchmarkConfig '>"
    benchmark_plugin_exception_0 = module_0.BenchmarkPluginException(str_1)
    var_3 = benchmark_config_0.get_cmd()
    assert var_3 == "wrk2 ' -c 1 -t 48 -d 20 --timeout 20 -R1"
    host_data_0 = module_0.HostData()
    assert host_data_0.cpu_count == 48
    assert host_data_0.env == {'PYNGUIN_DANGER_AWARE': '', 'PYTHONUNBUFFERED': '1', 'HOSTNAME': '493e3850223b', 'PYTHON_VERSION': '3.10.6', 'PYTHONHASHSEED': '0', 'PWD': '/pynguin', 'PYTHON_SETUPTOOLS_VERSION': '63.2.0', 'container': 'podman', 'HOME': '/root', 'LANG': 'C.UTF-8', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'TERM': 'xterm', 'SHLVL': '1', 'PYNGUIN_VERSION': '0.27.0', 'PYTHON_PIP_VERSION': '22.2.1', 'PYTHONDONTWRITEBYTECODE': '1', 'PYTHON_GET_PIP_SHA256': '5aefe6ade911d997af080b315ebcb7f882212d070465df544e1175ac2be519b4', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/5eaac1050023df1f5c98b173b248c260023f2278/public/get-pip.py', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', '_': '/usr/local/bin/pynguin'}
    with pytest.raises(module_0.ProcessBenchmarkException):
        benchmark_0.run(benchmark_0)

@pytest.mark.xfail(strict=True)
def test_case_26():
    str_0 = '*Q'
    missing_dependency_exception_0 = module_0.MissingDependencyException()
    none_type_0 = None
    module_0.BenchmarkSuite(none_type_0, none_type_0, str_0, _id=str_0, metadata=none_type_0, end_time=none_type_0)

@pytest.mark.xfail(strict=True)
def test_case_27():
    str_0 = 'a5F59`\x0bM$'
    none_type_0 = None
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    str_1 = "'"
    bool_0 = True
    str_2 = 'g=W!$dWW^Sudniv\\'
    benchmark_config_0 = module_0.BenchmarkConfig(str_1, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_2, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    bool_1 = True
    bool_2 = True
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == "wrk2 ' -c 1 -t 48 -d 20 --timeout 20 -R1"
    str_3 = '6Cq]V9iaW>@_2ztB'
    benchmark_config_1 = module_0.BenchmarkConfig(str_3, bool_1, bool_2, app_variant=str_0, responses_per_second=bool_0, goals=none_type_0)
    assert f'{type(benchmark_config_1).__module__}.{type(benchmark_config_1).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = benchmark_config_0.get_cmd()
    assert var_1 == "wrk2 ' -c 1 -t 48 -d 20 --timeout 20 -R1"
    int_0 = 4
    module_0.BenchmarkSuite(var_1, str_2, var_1, think_time=int_0, benchmarks_ids=bool_2, metadata=var_1, host_data=var_0)

@pytest.mark.xfail(strict=True)
def test_case_28():
    str_0 = 'a5F59`\x0bM$'
    host_data_0 = module_0.HostData(str_0)
    assert host_data_0.cpu_count == 'a5F59`\x0bM$'
    assert host_data_0.env == {'PYNGUIN_DANGER_AWARE': '', 'PYTHONUNBUFFERED': '1', 'HOSTNAME': '493e3850223b', 'PYTHON_VERSION': '3.10.6', 'PYTHONHASHSEED': '0', 'PWD': '/pynguin', 'PYTHON_SETUPTOOLS_VERSION': '63.2.0', 'container': 'podman', 'HOME': '/root', 'LANG': 'C.UTF-8', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'TERM': 'xterm', 'SHLVL': '1', 'PYNGUIN_VERSION': '0.27.0', 'PYTHON_PIP_VERSION': '22.2.1', 'PYTHONDONTWRITEBYTECODE': '1', 'PYTHON_GET_PIP_SHA256': '5aefe6ade911d997af080b315ebcb7f882212d070465df544e1175ac2be519b4', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/5eaac1050023df1f5c98b173b248c260023f2278/public/get-pip.py', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', '_': '/usr/local/bin/pynguin'}
    none_type_0 = None
    var_0 = module_0.handle_plugins(none_type_0)
    benchmark_plugin_0 = module_0.BenchmarkPlugin(var_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert f'{type(benchmark_plugin_0.module).__module__}.{type(benchmark_plugin_0.module).__qualname__}' == 'builtins.generator'
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    str_1 = '2}U\n'
    bool_0 = True
    bool_1 = True
    benchmark_config_0 = module_0.BenchmarkConfig(str_1, bool_0, timeout=bool_1, script=none_type_0, test_id=var_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = benchmark_config_0.__repr__()
    assert var_1 == '<BenchmarkConfig 2}U>'
    var_2 = benchmark_config_0.get_cmd()
    assert var_2 == 'wrk 2}U -c 10 -t 1 -d 20 --timeout 1 --latency'
    module_0.BenchmarkSuite(var_0, benchmark_config_0, str_1, benchmark_plugin_0, host_data=var_1)

@pytest.mark.xfail(strict=True)
def test_case_29():
    str_0 = 'a5F59`\x0bM$'
    none_type_0 = None
    var_0 = module_0.handle_plugins(none_type_0)
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, responses_per_second=none_type_0, latency_statistics=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = benchmark_config_0.__repr__()
    assert var_1 == '<BenchmarkConfig a5F59`\x0bM$>'
    int_0 = 0
    str_1 = 'FXYVlSIg+9\ro9*'
    str_2 = '"J|Xx\x0c5W)5'
    process_benchmark_exception_0 = module_0.ProcessBenchmarkException(str_2, var_1)
    var_2 = benchmark_config_0.get_cmd()
    assert var_2 == 'wrk a5F59`\x0bM$ -c 10 -t 48 -d 20 --timeout 20'
    host_data_0 = module_0.HostData(env=int_0)
    assert host_data_0.cpu_count == 48
    assert host_data_0.env == 0
    none_type_1 = None
    str_3 = 'K039\\k{'
    benchmark_plugin_exception_0 = module_0.BenchmarkPluginException(str_3)
    var_3 = benchmark_config_0.get_cmd()
    assert var_3 == 'wrk a5F59`\x0bM$ -c 10 -t 48 -d 20 --timeout 20'
    str_4 = '57hO./t_4):`qGqq@x'
    process_benchmark_exception_1 = module_0.ProcessBenchmarkException(str_4, var_2)
    var_4 = benchmark_config_0.get_cmd()
    assert var_4 == 'wrk a5F59`\x0bM$ -c 10 -t 48 -d 20 --timeout 20'
    module_0.BenchmarkSuite(process_benchmark_exception_1, var_0, str_1, think_time=none_type_0, benchmarks_ids=process_benchmark_exception_1, end_time=none_type_1)

@pytest.mark.xfail(strict=True)
def test_case_30():
    str_0 = 'a5F59`\x0bM$'
    none_type_0 = None
    var_0 = module_0.handle_plugins(none_type_0)
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, responses_per_second=none_type_0, latency_statistics=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = benchmark_config_0.__repr__()
    assert var_1 == '<BenchmarkConfig a5F59`\x0bM$>'
    none_type_1 = None
    dict_0 = {str_0: benchmark_config_0, none_type_1: var_1}
    benchmark_plugin_1 = module_0.BenchmarkPlugin(dict_0)
    bool_0 = False
    module_0.BenchmarkConfig(dict_0, none_type_0, var_1, timeout=bool_0, headers=benchmark_plugin_1, latency_statistics=none_type_1)

@pytest.mark.xfail(strict=True)
def test_case_31():
    none_type_0 = None
    var_0 = module_0.handle_plugins(none_type_0)
    benchmark_plugin_0 = module_0.BenchmarkPlugin(var_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert f'{type(benchmark_plugin_0.module).__module__}.{type(benchmark_plugin_0.module).__qualname__}' == 'builtins.generator'
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    str_0 = '@|\'!O\nyZ1]2:J"z\x0cW'
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, headers=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = var_0.__repr__()
    var_2 = benchmark_plugin_0.to_dict()
    none_type_1 = None
    bool_0 = True
    performance_goal_result_0 = module_0.PerformanceGoalResult(bool_0, none_type_1)
    assert f'{type(performance_goal_result_0).__module__}.{type(performance_goal_result_0).__qualname__}' == 'wrktoolbox.benchmarks.PerformanceGoalResult'
    assert performance_goal_result_0.success is True
    assert performance_goal_result_0.goal is None
    assert performance_goal_result_0.error is None
    var_3 = module_1.full_load_all(benchmark_config_0)
    var_3.to_dict()

@pytest.mark.xfail(strict=True)
def test_case_32():
    str_0 = 'zO{BZ'
    performance_goal_result_0 = module_0.PerformanceGoalResult(str_0, str_0, str_0)
    assert f'{type(performance_goal_result_0).__module__}.{type(performance_goal_result_0).__qualname__}' == 'wrktoolbox.benchmarks.PerformanceGoalResult'
    assert performance_goal_result_0.success == 'zO{BZ'
    assert performance_goal_result_0.goal == 'zO{BZ'
    assert performance_goal_result_0.error == 'zO{BZ'
    int_0 = -935
    var_0 = performance_goal_result_0.to_dict()
    module_0.BenchmarkConfig(str_0, int_0, int_0, goals=int_0)

@pytest.mark.xfail(strict=True)
def test_case_33():
    str_0 = 'nK\x0c[HoI=.)a9WuVvFJ}'
    none_type_0 = None
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    str_1 = "'"
    bool_0 = True
    str_2 = 'g=W!$dWW^Sudniv\\'
    benchmark_config_0 = module_0.BenchmarkConfig(str_1, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_2, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    bool_1 = True
    bool_2 = True
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == "wrk2 ' -c 1 -t 48 -d 20 --timeout 20 -R1"
    str_3 = '6Cq]V9iaW>@_2ztB'
    benchmark_config_1 = module_0.BenchmarkConfig(str_3, bool_1, bool_2, app_variant=str_0, responses_per_second=bool_0, goals=none_type_0)
    assert f'{type(benchmark_config_1).__module__}.{type(benchmark_config_1).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_1 = benchmark_config_0.__repr__()
    assert var_1 == "<BenchmarkConfig '>"
    str_4 = 'threads'
    benchmark_plugin_exception_0 = module_0.BenchmarkPluginException(str_4)
    str_5 = ''
    str_6 = 'Vh8vj[BXbW}x{'
    str_7 = 'Omes(K&?{`xz_'
    str_8 = 'xTOmM"3pI9=\'7@l\nt/'
    dict_0 = {str_5: str_6, str_7: str_8}
    benchmark_config_2 = module_0.BenchmarkConfig(bool_2, timeout=bool_1, headers=dict_0, repeat=bool_0)
    assert f'{type(benchmark_config_2).__module__}.{type(benchmark_config_2).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    var_2 = benchmark_config_2.get_cmd()
    assert var_2 == 'wrk True -c 10 -t 48 -d 20 --timeout 1 --latency -H ": Vh8vj[BXbW}x{" -H "Omes(K&?{`xz_: xTOmM"3pI9=\'7@l\nt/"'
    int_0 = -2722
    module_0.BenchmarkSuite(var_1, var_2, str_5, think_time=int_0, public_ip=str_1, end_time=str_0)

def test_case_34():
    str_0 = 'a5F59`\x0bM$'
    none_type_0 = None
    benchmark_plugin_0 = module_0.BenchmarkPlugin(none_type_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module is None
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    str_1 = "'"
    bool_0 = True
    str_2 = 'g=W!$dWW^Sudniv\\'
    benchmark_config_0 = module_0.BenchmarkConfig(str_1, concurrency=bool_0, responses_per_second=bool_0, latency_statistics=none_type_0, test_id=str_2, goals=none_type_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    bool_1 = True
    bool_2 = True
    var_0 = benchmark_config_0.get_cmd()
    assert var_0 == "wrk2 ' -c 1 -t 48 -d 20 --timeout 20 -R1"
    benchmark_0 = module_0.Benchmark(benchmark_config_0)
    assert f'{type(benchmark_0).__module__}.{type(benchmark_0).__qualname__}' == 'wrktoolbox.benchmarks.Benchmark'
    str_3 = ''
    str_4 = '6Cq]V9iaW>@_2ztB'
    benchmark_config_1 = module_0.BenchmarkConfig(str_4, bool_1, bool_2, app_variant=str_0, responses_per_second=bool_0, goals=none_type_0)
    assert f'{type(benchmark_config_1).__module__}.{type(benchmark_config_1).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    benchmark_plugin_exception_0 = module_0.BenchmarkPluginException(str_3)
    var_1 = benchmark_config_0.get_cmd()
    assert var_1 == "wrk2 ' -c 1 -t 48 -d 20 --timeout 20 -R1"
    host_data_0 = module_0.HostData()
    assert host_data_0.cpu_count == 48
    assert host_data_0.env == {'PYNGUIN_DANGER_AWARE': '', 'PYTHONUNBUFFERED': '1', 'HOSTNAME': '493e3850223b', 'PYTHON_VERSION': '3.10.6', 'PYTHONHASHSEED': '0', 'PWD': '/pynguin', 'PYTHON_SETUPTOOLS_VERSION': '63.2.0', 'container': 'podman', 'HOME': '/root', 'LANG': 'C.UTF-8', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'TERM': 'xterm', 'SHLVL': '1', 'PYNGUIN_VERSION': '0.27.0', 'PYTHON_PIP_VERSION': '22.2.1', 'PYTHONDONTWRITEBYTECODE': '1', 'PYTHON_GET_PIP_SHA256': '5aefe6ade911d997af080b315ebcb7f882212d070465df544e1175ac2be519b4', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/5eaac1050023df1f5c98b173b248c260023f2278/public/get-pip.py', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', '_': '/usr/local/bin/pynguin'}
    with pytest.raises(module_0.ProcessBenchmarkException):
        benchmark_0.run()

@pytest.mark.xfail(strict=True)
def test_case_35():
    wrk_variant_0 = module_0.WrkVariant.WRK2
    none_type_0 = None
    str_0 = '<~`Cd".'
    bool_0 = True
    benchmark_config_0 = module_0.BenchmarkConfig(str_0, bool_0, app_variant=wrk_variant_0, headers=none_type_0, latency_statistics=bool_0, repeat=bool_0)
    assert f'{type(benchmark_config_0).__module__}.{type(benchmark_config_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkConfig'
    module_2.decodebytes(wrk_variant_0)

@pytest.mark.xfail(strict=True)
def test_case_36():
    wrk_variant_0 = module_0.WrkVariant.WRK2
    none_type_0 = None
    int_0 = -1594
    benchmark_plugin_0 = module_0.BenchmarkPlugin(wrk_variant_0)
    assert f'{type(benchmark_plugin_0).__module__}.{type(benchmark_plugin_0).__qualname__}' == 'wrktoolbox.benchmarks.BenchmarkPlugin'
    assert benchmark_plugin_0.module == module_0.WrkVariant.WRK2
    assert f'{type(module_0.BenchmarkPlugin.setup).__module__}.{type(module_0.BenchmarkPlugin.setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.has_setup).__module__}.{type(module_0.BenchmarkPlugin.has_setup).__qualname__}' == 'builtins.property'
    assert f'{type(module_0.BenchmarkPlugin.name).__module__}.{type(module_0.BenchmarkPlugin.name).__qualname__}' == 'builtins.property'
    bool_0 = True
    str_0 = "[? Q.XnAD#'a"
    module_0.BenchmarkConfig(str_0, int_0, bool_0, app_variant=wrk_variant_0, responses_per_second=bool_0, latency_statistics=none_type_0, repeat=benchmark_plugin_0, goals=none_type_0)